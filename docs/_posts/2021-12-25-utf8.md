---
title:  "The down and dirty of UTF-8"
date:   2021-11-03 16:36:48 +0100
categories: encoding engineering computer-science utf-8
---

## Unicode

How about collecting, categorizing and indexing all characters from all human
languages is a way that everyone uses only one encoding? Screw this 256 limit
and give every human character a number, for example:

- a ‚Üí decimal 97 ‚â° U+61 (hexadecimal notation, much more usual.)
- b ‚Üí decimal 98 ‚â° U+62
- A ‚Üí decimal 41 ‚â° U+29
- B ‚Üí decimal 42 ‚â° U+2A

- Œ≥ ‚Üí decimal 947 ‚â° U+3B3
- Œì ‚Üí decimal 915 ‚â° U+393

- Áà± ‚Üí code point decimal 24859 ‚â° U+611b

and so on. Just list them all!

That is the challenge that a few individuals in the end of the 80s accepted
and called their character map Unicode. For the sake of computers though,
they promised to not assign more than 1‚Åü112‚Åü064 characters, as a hugely
overestimated upper limit (a little more that the previous 255.)

Unicode assigns a "code point" to each character in their "character
repertoir".  Unicode 14.0 includes 144‚Åü697 characters from pretty much any
languange we can think of. Best of all (but not controversy-free), the first
127 characters are exactly the same as defined by ASCII. Less impressively,
however, characters 128 through 255 are taken from Latin1 (this causes some
complications that we will talk about soon).

So! Problem solved, right? Just transform the Unicode code point into binary
and lets live on! Not so fast‚Ä¶ ASCII was one byte long, but we need 3 bytes for
encoding the number 144‚Åü697, but only one byte to encode the letter "A" ( code
point 65). If we need 3 bytes for all ASCII characters we would have a torrent
of zero bytes taking up space in our hard drives. Imagine trippling the storage
taken by all text in English when coverting from ASCII to Unicode? This was a
problem that has a brilliant solution called UTF-8.

We will describe UTF-8 in detail, but for now, let's put Unicode into scrutiny
with a magnifying glass üîé. Or maybe üïµüèΩ‚Äç‚ôÇÔ∏è, to keep things interesting
and cohesive with my Brazilian background.


## UTF-8

If simply converting a code point to binary isn't smart, then we need something
better.

Note: code points are spoken of in hexadecimal instead of decimal, so from now
on, instead of 127, we will say `0x7F`. Same same.

The trick of UTF-8 is to separate the character table from the encoding. It is
no longer simple to convert the binary encoding into a code point number and
vice-versa.

Here is how it works:

From `0x0` to `0x7F`, do the simple thing: convert to binary. This results in bytes
with the most significant bit always zeroed.

The number `0x0` in binary is:
```text
bit   | 0123457 |
value | 0000000 |
```

And the number `0x7F` (127, remember?) in binary is
```text
      | Byte 1  |
bit   | 0123457 |
value | 0111111 |
```

So Unicode decoders see a character byte starting with `0`, it already knows that
this is a single byte character: job done, next character.

As soon as the most significant bit turns into 1, we switch modes: we have a
multi-byte characters:

![UTF-8 control bytes](/resources/UTF-8/UTF-8.png){:class="img-responsive"}

Multi-byte characters have a first byte starting with `110`, `1110` or `11110` for
two, three or four byte characters. The continuation bytes always start with `10`,
which clearly informs decoders of its function. All other available bits (marked
with an `x`) encode the code point value in binary.

Examples:

The snowman character ‚òÉ is code point U+2603. When encoded in UTF-8 it is
represented by the `0xE2 96 83`.

![UTF-8 Snowman](/resources/UTF-8/Snowman-UTF-8.png){:class="img-responsive"}

The peach character üçë is code point U+1F351. Its UTF-8 encoded representation
is `0xF0 9F 8D 91`:

![UTF-8 Peach](/resources/UTF-8/Peach-UTF-8.png){:class="img-responsive"}

Doing those conversions manually make it clear that the UTF-8 encoding results
in a hexadecimal number that is completely different from the hexadecimal value
of the code point: `U+2603` becomes `0xE2 0x96 0x83`.

So UTF-8 is "storage smart" because it is smart about the amount of bytes that
it takes to store a character. However, it is not so "processing" smart. In
order to figure out the length of a string, it is necessary to traverse the
entire string, resulting in a linear growth rate (O(n)).


## BOM ‚Äì Byte Order Mark

TODO
