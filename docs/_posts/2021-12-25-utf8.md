---
title:  "The down and dirty of UTF-8"
date:   2021-11-03 16:36:48 +0100
categories: encoding engineering computer-science utf-8
---

What the double UTF is UTF-8? Let's lay it down, bit-by-bit.


## Unicode and UTF-8

In my [previous post]({% post_url 2021-11-21-character-encoding %}) we dug a little deeper
into the reasons to be for Unicode, and found out that Unicode is an imense
character table containing all the charaters in the world. Or at least an
a very good attempt of that. This table has 1‚Åü112‚Åü064 rows, and each row
is a **code point** (i.e. a number), but "only" 144‚Åü697 of code points have
been assigned so far. Each one of these code points
[represents a character/symbol](http://www.unexpected-vortices.com/misc-notes/text-unicode/terminology.html).

Now let's get the highest code point on this table and convert to binary:

```
1 112 064 ‚Üí 0b10000 11111000 00000000
```

Well, there you go, that's how we can store code points in a computer, right?

Sure, congratulations you've just invented UTF-32. Well, you've actually
invented UTF-24, which was proposed, but good luck finding a 24 bit data type
with good similar performance as 16 and 32 bits data types.

So the text "hello world!" in UTF-32 is:

character | decimal | UTF-32
---|---|---
h | 104 | `0x00 00 00 68`
e | 101 | `0x00 00 00 65`
l | 108 | `0x00 00 00 6c`
l | 108 | `0x00 00 00 6c`
o | 111 | `0x00 00 00 6f`
  | 032 | `0x00 00 00 20`
w | 119 | `0x00 00 00 77`
o | 111 | `0x00 00 00 6f`
r | 114 | `0x00 00 00 72`
l | 108 | `0x00 00 00 6c`
d | 100 | `0x00 00 00 64`
! | 033 | `0x00 00 00 21`

It is pretty clear that for a simply "hello world!" text UTF-32 is using 44
bytes, wereas ASCII would use only 11. That is a 400% increase, and nobody in
their sane minds would start storing their text files in a format that is so
extremely space inneficient.


## UTF-8

If simply converting a code point to binary isn't smart, then we need something
better.

The stroke of genius was to separate the character table (Unicode) from the
encoding. UTF-8 is a character encoding that allows for a variable number of
bytes: any UTF-8 encoded character can have 1, 2, 3 or 4 bytes. And that
information is baked into the encoding itself.

Here is how it works:

From `0x0` to `0x7F` (i.e. from 0 to 127), do the simple thing: convert to
binary. This results in bytes with the most significant bit always zeroed,
because 127 in binary is `0b0111 1111`.

So Unicode decoders see a character byte starting with `0`, it already knows that
this is a single byte character: job done, next character.

Now, if most significant bit is 1, we switch modes: we have a multi-byte
characters:

![UTF-8 control bytes](/resources/UTF-8/UTF-8.png){:class="img-responsive"}

Multi-byte characters have a first byte starting with `110`, `1110` or `11110` for
two, three or four-byte characters. The continuation bytes always start with `10`,
which clearly informs decoders of its function. All other available bits (marked
with an `x`) encode the code point value in binary.

Examples:

The snowman character ‚òÉ is code point U+2603. When encoded in UTF-8 it is
represented by the `0xE2 96 83`.

![UTF-8 Snowman](/resources/UTF-8/Snowman-UTF-8.png){:class="img-responsive"}

The peach character üçë is code point U+1F351. Its UTF-8 encoded representation
is `0xF0 9F 8D 91`:

![UTF-8 Peach](/resources/UTF-8/Peach-UTF-8.png){:class="img-responsive"}

Doing those conversions manually make it clear that the UTF-8 encoding results
in a hexadecimal number that is completely different from the hexadecimal value
of the code point: `U+2603` becomes `0xE2 0x96 0x83`.

So UTF-8 is "storage smart" because it is smart about the amount of bytes that
it takes to store a character. However, it is not so "processing" smart. In
order to figure out the length of a string, it is necessary to traverse the
entire string, resulting in a linear growth rate (O(n)).


## BOM ‚Äì Byte Order Mark

UTF-8 does not need a BOM due to its variable character length. However,
Windows (ah, Windows) wants to add  `0xEF BB 0xBF` to the beginning of UTF-8
encoded files. Mac and Linux do not do that. So sending a file over to your
boss (why do bosses always use Windows?) and receiving it back in your Linux
computer will add some garbage to the beginning of your file. And that might
cause you problems if you are not aware. And if that garbage looks like √Ø¬ª¬ø
you might want to read my 
[general article about character encodings]({% post_url 2021-11-21-character-encoding %}).

Also make sure to read up on [UTF-16]({% post_url 2021-12-25-utf16 %}) to
make sure you're fully rounded on Unicode Transformation Formats.
